{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Lab 9: Introduction to NLP\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dx_-OQ2ufhpd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NAME: RAJA HAIDER ALI\n",
        "# CMS ID: 346900\n",
        "# GROUP: 02"
      ],
      "metadata": {
        "id": "5uRt6TcNfaq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk==3.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHp8_bWef1Fj",
        "outputId": "f7b1ad0a-a88d-4ebe-a053-35d9f2e99278"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk==3.5 in /usr/local/lib/python3.10/dist-packages (3.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk==3.5) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk==3.5) (1.3.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from nltk==3.5) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk==3.5) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "tudNmEWZhliB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task1 (Tokenization)\n",
        "Tokenization means either to split a paragraph into words or into sentences. NLTK has built\u0002in functions which can perform both of these types of tokenization.\n",
        "In task 1 you are given an example string and your task is to convert it into token both with\n",
        "respect to words and sentences."
      ],
      "metadata": {
        "id": "A6QVYL3FfvzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAUEYUdqiMQ6",
        "outputId": "8597a715-f5cd-4d48-85fe-98b9baaad508"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_string = \"\"\"\n",
        "Muad'Dib learned rapidly because his first training was in how to learn.\n",
        "And the first lesson of all was the basic trust the he could learn.\n",
        "It's shocking to find how many people do not believe they can learn.\n",
        "and how many more believe learning to be difficult.\"\"\""
      ],
      "metadata": {
        "id": "zklmlqfhffwD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# tokenizing into sentences\n",
        "sentences = sent_tokenize(example_string)\n",
        "\n",
        "# tokenizing into words\n",
        "words = word_tokenize(example_string)"
      ],
      "metadata": {
        "id": "Iot02xKHgzeO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kyw5xEC7hdna",
        "outputId": "29740b51-adf5-4cb3-a053-bc3d26f1fe03"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'And the first lesson of all was the basic trust the he could learn.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PEgNjqZ2iQ3K",
        "outputId": "eaea7c2d-b517-453a-c0c8-c084f5eda559"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Muad'Dib\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task2 (Filtering Stop words):-\n",
        "Stop words are words that you want to ignore, so you filter them out of your text when\n",
        "you’re processing it. Very common words like 'in', 'is', and 'an' are often used as stop words\n",
        "since they don’t add a lot of meaning to a text in and of themselves.\n",
        "Here’s how to import the relevant parts of NLTK in order to filter out stop words"
      ],
      "metadata": {
        "id": "soY3Gwr0ir5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MY74vG8jiUma",
        "outputId": "e83a6707-cf5c-4901-cdc3-5004d2bd3aff"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_quote = \"Sir, I protest. I am not a merry man\""
      ],
      "metadata": {
        "id": "a5nNlUF8jBCF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizing into words\n",
        "words = word_tokenize(word_quote)\n",
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGI4pdWGjGK6",
        "outputId": "24f718ab-943a-41b4-e993-c54c6b6dd05d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Sir', ',', 'I', 'protest', '.', 'I', 'am', 'not', 'a', 'merry', 'man']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "english_stops = set(stopwords.words('english'))\n",
        "for word in english_stops:\n",
        "  if \"I\" in word:\n",
        "    print(\"present\")"
      ],
      "metadata": {
        "id": "qUP41srJjTGN"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Hd722pjwsPC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_list = [word for word in words if word not in english_stops]\n",
        "filtered_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cutKxHX2ji7R",
        "outputId": "75abb516-cca3-44ac-f165-a245d74392da"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Sir', ',', 'I', 'protest', '.', 'I', 'merry', 'man']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(filtered_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aUfcrGPj9W2",
        "outputId": "326ba116-96c5-4178-f68c-192d1405a9b3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Sir', ',', 'I', 'protest', '.', 'I', 'merry', 'man']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task3 (Stemming):-\n",
        "Stemming is a text processing task in which you reduce words to their root, which is the core\n",
        "part of a word. For example, the words “helping” and “helper” share the root “help.”\n",
        "Stemming allows you to zero in on the basic meaning of a word rather than all the details of\n",
        "how it’s being used. NLTK has more than one stemmer, but you’ll be using the Porter"
      ],
      "metadata": {
        "id": "VIkujPTCkFpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "VRisKGFFkTU4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "string_for_stemming = \"\"\"\n",
        "The crew of the USS Discovery discovered many discoveries.\n",
        "Discovering is what explorers do.\"\"\""
      ],
      "metadata": {
        "id": "kt7mMAmSkcgr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ps = PorterStemmer()"
      ],
      "metadata": {
        "id": "aZRyGTrXkml8"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizing into words\n",
        "words = word_tokenize(string_for_stemming)\n",
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ednH-JVolDrA",
        "outputId": "7615237d-94fb-4ab8-af93-ee2cb97780a0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'crew',\n",
              " 'of',\n",
              " 'the',\n",
              " 'USS',\n",
              " 'Discovery',\n",
              " 'discovered',\n",
              " 'many',\n",
              " 'discoveries',\n",
              " '.',\n",
              " 'Discovering',\n",
              " 'is',\n",
              " 'what',\n",
              " 'explorers',\n",
              " 'do',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lst = []\n",
        "for word in words:\n",
        "  stem =ps.stem(word)\n",
        "  lst.append(stem)\n",
        "lst"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYGE1Elil89n",
        "outputId": "0b6c7753-d1c6-4f72-cf53-940c48968303"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the',\n",
              " 'crew',\n",
              " 'of',\n",
              " 'the',\n",
              " 'uss',\n",
              " 'discoveri',\n",
              " 'discov',\n",
              " 'mani',\n",
              " 'discoveri',\n",
              " '.',\n",
              " 'discov',\n",
              " 'is',\n",
              " 'what',\n",
              " 'explor',\n",
              " 'do',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task5 (Lemmatizing):-\n",
        "Now that you’re up to speed on parts of speech, you can circle back to lemmatizing. Like\n",
        "stemming, lemmatizing reduces words to their core meaning, but it will give you a complete\n",
        "English word that makes sense on its own instead of just a fragment of a word like 'discoveri'.\n",
        "Given the above strin your task is to lemmatize it to obtain the following output."
      ],
      "metadata": {
        "id": "rJFjdKWVmrw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "string_for_lemmatizing = \"The friends of DeSoto love scarves.\""
      ],
      "metadata": {
        "id": "PINL5yh9mfHK"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiINGYO4oOTj",
        "outputId": "e0d8c9c9-4fb3-45d8-d941-7dfb4fb24ee4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "3CWfA0OCm-6z"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizing into words\n",
        "words = word_tokenize(string_for_lemmatizing)\n",
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAW7hPJInj9A",
        "outputId": "9e70b611-2f65-4fe2-f35d-cd9d8927cf4f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'friends', 'of', 'DeSoto', 'love', 'scarves', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lst=[]\n",
        "\n",
        "for word in words:\n",
        "  lem = lemmatizer.lemmatize(word)\n",
        "  lst.append(lem)\n",
        "print(lst)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cz2IRvW9nzkC",
        "outputId": "ef834b79-d298-48b9-e129-798fd1359174"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'friend', 'of', 'DeSoto', 'love', 'scarf', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task6 (Chunking):-\n",
        "Chunking is defined as the process of natural language processing used to identify parts of\n",
        "speech and short phrases present in a given sentence."
      ],
      "metadata": {
        "id": "V1QYaXLEoYRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lotr_quote = \"It's a dangerous business, Frodo, going out your door\""
      ],
      "metadata": {
        "id": "Vug7tYa_oCDS"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag, RegexpParser"
      ],
      "metadata": {
        "id": "IDs5CYjWootV"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfV9JxVLpz2x",
        "outputId": "8e1a5538-9057-4aff-977b-cdbbd755f24e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizing into words\n",
        "words = word_tokenize(lotr_quote)\n",
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofOTzfxPo-nW",
        "outputId": "b509d608-32b3-4d23-cb16-39a45e4a75e1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['It',\n",
              " \"'s\",\n",
              " 'a',\n",
              " 'dangerous',\n",
              " 'business',\n",
              " ',',\n",
              " 'Frodo',\n",
              " ',',\n",
              " 'going',\n",
              " 'out',\n",
              " 'your',\n",
              " 'door']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Tag the words by part of speech.\n",
        "pos_tags = pos_tag(words)\n",
        "\n",
        "# Step 3: Define a chunk grammar.\n",
        "chunk_grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "\n",
        "# Step 4: Create a chunk parser with the defined grammar.\n",
        "chunk_parser = RegexpParser(chunk_grammar)\n",
        "\n",
        "# Step 5: Parse the tagged output through the parser.\n",
        "tree = chunk_parser.parse(pos_tags)\n",
        "\n",
        "# Display the resulting tree\n",
        "print(tree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQUfbNAQpC0G",
        "outputId": "a9d7e5a1-1dfc-4de8-b559-6e63cb8bd16a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  It/PRP\n",
            "  's/VBZ\n",
            "  (NP a/DT dangerous/JJ business/NN)\n",
            "  ,/,\n",
            "  Frodo/NNP\n",
            "  ,/,\n",
            "  going/VBG\n",
            "  out/RP\n",
            "  your/PRP$\n",
            "  (NP door/NN))\n"
          ]
        }
      ]
    }
  ]
}